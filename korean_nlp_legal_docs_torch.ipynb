{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 설명\n",
    "법률 문서에 대한 원문을 가장 잘 나타내는 3개의 문장을 추출하는 문서 요약 모델 개발\n",
    "    \n",
    "### 데이터 설명\n",
    "- 개요 : 뉴스 기사, 기고문, 잡지, 법률 (판결문) 등 다양한 영역에서 추출된 텍스트 데이터와 요약본 40만 건\n",
    "- 입출력: \n",
    "    - 입력: 문장별로 나뉜 법률 문서 원문, 예) 법률 문서 = [문장1, 문장2, ..., 문장K], K : 문서 길이\n",
    "    - 출력: 요약문에 포함될 문장 인덱스 3개\n",
    "- 데이터셋 구성\n",
    "    - 학습 데이터:\n",
    "        - train.json : 24,027개의 법률 문서 아이디 (id), 원문 (article_original), 요약문 (extractive)\n",
    "    - 테스트 데이터:\n",
    "        - test.json : 3,004개의 법률 문서 아이디 (id), 원문 (article_original)\n",
    "\n",
    "### 사용 pretrained 모델\n",
    " `beomi/KcELECTRA-base` \n",
    "[Documentation](https://github.com/Beomi/KcELECTRA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세팅\n",
    "### 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 설치되지 않은 라이브러리의 경우, 주석 해제 후 코드를 실행하여 설치\n",
    "# !pip install pytorch-pretrained-bert\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 및 코드 파일 불러오기\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 경로 설정\n",
    "ROOT_PATH = './'\n",
    "DATA_DIR = './data'\n",
    "MODEL_DIR = ROOT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "EVAL_BATCH_SIZE = 1 \n",
    "\n",
    "# 학습 데이터만 있으니 학습 데이터셋 비율과 validation 데이터셋 비율을 나눔\n",
    "TRAIN_RATIO = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__init__` 에서 tokenizer는 transformers 라이브러리에서 AutoTokenizer를 사용합니다. 이 외에도 원하는 토크나이저를 적용해 다양한 실험을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.inputs, self.labels = self.data_loader()\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        \n",
    "        if os.path.isfile(os.path.join(self.data_dir, self.mode + '_X.pt')):\n",
    "            inputs = torch.load(os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "            labels = torch.load(os.path.join(self.data_dir, self.mode + '_Y.pt'))\n",
    "\n",
    "        else:\n",
    "            file_path = os.path.join(self.data_dir, 'train.json')\n",
    "            df = pd.read_json(file_path, orient='records', encoding='utf-8-sig')\n",
    "          \n",
    "            if self.mode == 'train':\n",
    "                df = df.loc[:TRAIN_RATIO*int(len(df)), :]\n",
    "            elif self.mode == 'val':\n",
    "                df = df.loc[TRAIN_RATIO*int(len(df)):, :]\n",
    "\n",
    "            inputs = pd.DataFrame(columns=['src'])\n",
    "            labels = pd.DataFrame(columns=['trg'])\n",
    "            inputs['src'] =  df['article_original']\n",
    "            labels['trg'] =  df['extractive']\n",
    "          \n",
    "            # Preprocessing\n",
    "            inputs, labels = self.preprocessing(inputs, labels)\n",
    "            # Save data\n",
    "            torch.save(inputs ,os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "            torch.save(labels, os.path.join(self.data_dir, self.mode + '_Y.pt'))\n",
    "\n",
    "        inputs = inputs.values\n",
    "        labels = labels.values\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def pad(self, data, pad_id, max_len):\n",
    "        padded_data = data.map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]))\n",
    "        return padded_data\n",
    "\n",
    "    def preprocessing(self, inputs, labels):\n",
    "        print('Preprocessing ' + self.mode + ' dataset..')\n",
    "        #Encoding original text\n",
    "        inputs['src'] = inputs['src'].map(lambda x: torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)),  add_special_tokens=True) for i in range(len(x))]))))\n",
    "        inputs['clss'] = inputs.src.map(lambda x : torch.cat([torch.where(x == 2)[0], torch.tensor([len(x)])]))\n",
    "        inputs['segs'] = inputs.clss.map(lambda x : torch.tensor(list(chain.from_iterable([[0] * (x[i+1] - x[i]) if i % 2 == 0 else [1] * (x[i+1] - x[i]) for i, val in enumerate(x[:-1])]))))\n",
    "        inputs['clss'] = inputs.clss.map(lambda x : x[:-1])\n",
    "\n",
    "        ##Padding\n",
    "        max_encoding_len = max(inputs.src.map(lambda x: len(x)))\n",
    "        max_label_len = max(inputs.clss.map(lambda x: len(x)))\n",
    "        inputs['src'] = self.pad(inputs.src, 0, max_encoding_len)\n",
    "        inputs['segs'] = self.pad(inputs.segs, 0, max_encoding_len)\n",
    "        inputs['clss'] = self.pad(inputs.clss, -1, max_label_len)\n",
    "        inputs['mask'] = inputs.src.map(lambda x: ~ (x == 0))\n",
    "        inputs['mask_clss'] = inputs.clss.map(lambda x: ~ (x == -1))\n",
    "\n",
    "        #Binarize label {Extracted sentence : 1, Not Extracted sentence : 0}\n",
    "        labels = labels['trg'].map(lambda  x: torch.tensor([1 if i in x else 0 for i in range(max_label_len)]))\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.inputs[index][i] for i in range(5)], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train')\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "EPOCHS = 1000 \n",
    "LEARNING_RATE = 0.0005\n",
    "WEIGHT_DECAY = 0.00001\n",
    "NUM_WORKERS = 0\n",
    "EARLY_STOPPING_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어 자연어 처리의 pretrained model인 KcELECTRA 깃헙 [https://github.com/Beomi/KcELECTRA] 참고\n",
    "- !주의! 모델이 무거우니 사용하는 파라미터 개수와 개발 환경 등을 고려하여 인코더 등을 선택하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import transformers\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.encoder = transformers.BertModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, segs, clss, mask, mask_clss, sentence_range=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        top_vec = self.encoder(input_ids = x.long(), attention_mask = mask.float(),  token_type_ids = segs.long()).last_hidden_state\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss.long()]\n",
    "        sents_vec = sents_vec * mask_clss[:, :, None].float()\n",
    "        h = self.fc(sents_vec).squeeze(-1)\n",
    "        sent_scores = self.sigmoid(h) * mask_clss.float()\n",
    "        return sent_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing BertModel: ['electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.11.output.LayerNorm.bias', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.11.intermediate.dense.bias', 'discriminator_predictions.dense.weight', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.embeddings.LayerNorm.weight', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.embeddings.position_ids', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.1.output.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.10.output.LayerNorm.weight', 'discriminator_predictions.dense.bias', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.5.intermediate.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.1.output.dense.bias', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.9.output.dense.weight', 'electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.output.dense.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.8.output.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hitrate(y_true, y_pred):\n",
    "    \"\"\" Metric 함수 반환하는 함수\n",
    "\n",
    "    Returns:\n",
    "        metric_fn (Callable)\n",
    "    \"\"\"\n",
    "    hitrate = np.array([len(list(set(ans).intersection(y_true[i])))/3 for i, ans in enumerate(y_pred)])\n",
    "    score = np.mean(hitrate)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "# Set metrics\n",
    "metric_fn = Hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int, verbose: bool, logger:logging.RootLogger=None)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.logger = logger\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\n",
    "\n",
    "        Args:\n",
    "            loss (float):\n",
    "\n",
    "        Examples:\n",
    "            \n",
    "        Note:\n",
    "            \n",
    "        \"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.save_model = True\n",
    "            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "            # self.save_checkpoint(loss=loss, model=model)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.logger.info(msg) if self.logger else print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \n",
    "    Attributes:\n",
    "        model (`model`)\n",
    "        device (str)\n",
    "        loss_fn (Callable)\n",
    "        metric_fn (Callable)\n",
    "        optimizer (`optimizer`)\n",
    "        scheduler (`scheduler`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, device, loss_fn, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.logger = logger\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.train_total_loss = 0\n",
    "        pred_lst = []\n",
    "        target_lst = []\n",
    "        for batch_index, (data, target) in enumerate(dataloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            src = data[0].to(self.device)\n",
    "            clss = data[1].to(self.device)\n",
    "            segs = data[2].to(self.device)\n",
    "            mask = data[3].to(self.device)\n",
    "            mask_clss = data[4].to(self.device)\n",
    "            target = target.float().to(self.device)\n",
    "            sent_score = self.model(src, segs, clss, mask, mask_clss)\n",
    "            loss = self.loss_fn(sent_score, target)\n",
    "            loss = (loss * mask_clss.float()).sum()\n",
    "            self.train_total_loss += loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            pred_lst.extend(torch.topk(sent_score, 3, axis=1).indices.tolist())\n",
    "            try:\n",
    "                target_lst.extend(torch.where(target==1)[1].reshape(-1,3).tolist())\n",
    "            except:\n",
    "                print(target)\n",
    "                sys.exit()\n",
    "                \n",
    "        self.train_mean_loss = self.train_total_loss / len(dataloader)\n",
    "        self.train_score = self.metric_fn(y_true=target_lst, y_pred=pred_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train, loss: {self.train_mean_loss}, Score: {self.train_score}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "\n",
    "        Args:\n",
    "            dataloader (`dataloader`)\n",
    "            epoch_index (int)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.val_total_loss = 0\n",
    "        pred_lst = []\n",
    "        target_lst = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (data, target) in enumerate(dataloader):\n",
    "                src = data[0].to(self.device)\n",
    "                clss = data[1].to(self.device)\n",
    "                segs = data[2].to(self.device)\n",
    "                mask = data[3].to(self.device)\n",
    "                mask_clss = data[4].to(self.device)\n",
    "                target = target.float().to(self.device)\n",
    "                sent_score = self.model(src, segs, clss, mask, mask_clss)\n",
    "                loss = self.loss_fn(sent_score, target)\n",
    "                loss = (loss * mask_clss.float()).sum()\n",
    "                self.val_total_loss += loss\n",
    "                pred_lst.extend(torch.topk(sent_score, 3, axis=1).indices.tolist())\n",
    "                target_lst.extend(torch.where(target==1)[1].reshape(-1,3).tolist())\n",
    "            self.val_mean_loss = self.val_total_loss / len(dataloader)\n",
    "            self.validation_score = self.metric_fn(y_true=target_lst, y_pred=pred_lst)\n",
    "            msg = f'Epoch {epoch_index}, Validation, loss: {self.val_mean_loss}, Score: {self.validation_score}'\n",
    "            print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set trainer\n",
    "trainer = Trainer(model, DEVICE, loss_fn, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]]), tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]]), tensor([[True, True, True, True, True, True, True]]), tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]]), tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]])] tensor([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (294) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-caa9a2fea7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-70010bc70dae>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmask_clss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0msent_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_clss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask_clss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-137f9a63e696>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, segs, clss, mask, mask_clss, sentence_range)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \"\"\"\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtop_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msents_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msents_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents_vec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask_clss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (294) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "criterion = 0\n",
    "\n",
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    trainer.train_epoch(train_dataloader, epoch_index=epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index=epoch_index)\n",
    "   \n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if trainer.validation_score > criterion:\n",
    "        criterion = trainer.validation_score\n",
    "        check_point:{\n",
    "            'model' : model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        \n",
    "        torch.save(check_point, os.path.join(MODEL_DIR, 'best.pt'))\n",
    "        \n",
    "        \n",
    "print(\"train finished, best.pt saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론\n",
    "테스트 데이터의 타겟 변수를 `sample_submission.csv` 양식에 맞춰 저장하고, 해당 제출파일을 Inclass에 제출하시면 점수를 확인할 수 있습니다.\n",
    "\n",
    "여러분의 모델의 추론 결과로 나온 문서 당 세 개의 요약 인덱스에 해당하는 \"idx_#\"을 1로 채워 제출 파일을 만듭니다(현재는 아래 보시는 바와 같이 모두 0으로 채워져 있습니다). ID 값을 기준으로 채점을 진행하는 점 유의해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(DATA_DIR,'sample_submission.csv'))\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 클래스 정의\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\" CustomDataset과 비슷한 구조이지만 레이블이 주어지지 않음을 염두 \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, mode):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.inputs = self.data_loader()\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if os.path.isfile(os.path.join(self.data_dir, self.mode+'_X.pt')):\n",
    "            # torch tensor 불러오기\n",
    "            inputs = torch.load(os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "        else:\n",
    "            # json 파일 불러오기\n",
    "            file_path = os.path.join(self.data_dir, self.mode + '.json')\n",
    "            df = pd.read_json(file_path, orient='records', encoding='utf-8-sig')\n",
    "            inputs = pd.DataFrame(columns=['src'])\n",
    "            inputs['src'] =  df['article_original']\n",
    "      \n",
    "            # 전처리\n",
    "            inputs = self.preprocessing(inputs)\n",
    "            \n",
    "            # 다음부터는 전처리 과정을 반복하지 않기 위해 tensor 저장\n",
    "            torch.save(inputs ,os.path.join(self.data_dir, self.mode + '_X.pt'))\n",
    "\n",
    "        inputs = inputs.values\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def pad(self, data, pad_id, max_len):\n",
    "        padded_data = data.map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]))\n",
    "        return padded_data\n",
    "\n",
    "    def preprocessing(self, inputs):\n",
    "        print('Preprocessing ' + self.mode + ' dataset..')\n",
    "        \n",
    "        #Encoding original text\n",
    "        inputs['src'] = inputs['src'].map(lambda x: torch.tensor(list(chain.from_iterable([self.tokenizer.encode(x[i], max_length = int(512 / len(x)),  add_special_tokens=True) for i in range(len(x))]))))\n",
    "        inputs['clss'] = inputs.src.map(lambda x : torch.cat([torch.where(x == 2)[0], torch.tensor([len(x)])]))\n",
    "        inputs['segs'] = inputs.clss.map(lambda x : torch.tensor(list(chain.from_iterable([[0] * (x[i+1] - x[i]) if i % 2 == 0 else [1] * (x[i+1] - x[i]) for i, val in enumerate(x[:-1])]))))\n",
    "        inputs['clss'] = inputs.clss.map(lambda x : x[:-1])\n",
    "\n",
    "        ##Padding\n",
    "        max_encoding_len = max(inputs.src.map(lambda x: len(x)))\n",
    "        max_label_len = max(inputs.clss.map(lambda x: len(x)))\n",
    "        inputs['src'] = self.pad(inputs.src, 0, max_encoding_len)\n",
    "        inputs['segs'] = self.pad(inputs.segs, 0, max_encoding_len)\n",
    "        inputs['clss'] = self.pad(inputs.clss, -1, max_label_len)\n",
    "        inputs['mask'] = inputs.src.map(lambda x: ~ (x == 0))\n",
    "        inputs['mask_clss'] = inputs.clss.map(lambda x: ~ (x == -1))\n",
    "     \n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return [self.inputs[index][i] for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, mode = 'test')\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" 이전에 학습한 모델 weight파일을 불러 추론하려면 아래 주석을 풀고 실행\n",
    "    학습 진행 후 바로 추론하는 경우 학습 과정의 model 사용 (주석 풀지 않고 실행) \"\"\"\n",
    "MODEL_DIR = os.path.join(ROOT_PATH, 'best.pt')\n",
    "model = Summarizer().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_DIR)['model'])\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "\n",
    "# 추론 결과를 pred 리스트로 저장할 예정\n",
    "pred_lst = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, (data) in enumerate(test_dataloader):\n",
    "        src = data[0].to(DEVICE)\n",
    "        clss = data[1].to(DEVICE)\n",
    "        segs = data[2].to(DEVICE)\n",
    "        mask = data[3].to(DEVICE)\n",
    "        mask_clss = data[4].to(DEVICE)\n",
    "        sent_score = model(src, segs, clss, mask, mask_clss)\n",
    "        pred_lst.extend(torch.topk(sent_score, 3, axis=1).indices.tolist())\n",
    "            \n",
    "        # 진행과정 출력\n",
    "        if batch_index % 150 == 0:\n",
    "            print(f'Prediction: {batch_index}/{len(test_dataloader)} completed')\n",
    "    print(\"Prediction all completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_lst[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일\n",
    "for i,txt in enumerate(pred_lst):\n",
    "    submit.iloc[i,txt[0]+1] += 1\n",
    "    submit.iloc[i,txt[1]+1] += 1\n",
    "    submit.iloc[i,txt[2]+1] += 1\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(os.path.join(ROOT_PATH, 'prediction.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.7 on Python 3.8 & CUDA 11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
